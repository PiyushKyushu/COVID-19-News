{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "CSV Preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiyushKyushu/COVID-19-News/blob/main/CSV_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPxPBtC6njpR"
      },
      "source": [
        "# For Articles\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from contractions import CONTRACTION_MAP\n",
        "import unicodedata\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "Japan = pd.read_csv(\"Britain_Articles.csv\")\n",
        "\n",
        "# CONTRACTION_MAP = {\"I'm\":\"I am\"} # contraction definition. This is just an example, please change it here with your contractions\n",
        "Japan[\"Articles\"] = Japan[\"Articles\"].apply(lambda x:str(x).replace(\"’\",\"'\")) # replace the wrong quotation mark by the correct one\n",
        "for contraction in CONTRACTION_MAP:\n",
        "    Japan[\"Articles\"] = Japan[\"Articles\"].apply(lambda x:str(x).replace(contraction, CONTRACTION_MAP[contraction])) # in this case I'm just replacing the contraction by the expanded form. I iterate it through all the possible contractions\n",
        "\n",
        "\n",
        "def strip_html_tags(row):\n",
        "    Japan_1 = row['Articles']\n",
        "    soup = BeautifulSoup(Japan_1, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "Japan['No_HTML'] = Japan.apply(strip_html_tags, axis=1)\n",
        "\n",
        "\n",
        "def identify_tokens(row):\n",
        "    articles = row['No_HTML']\n",
        "    token = nltk.word_tokenize(articles)\n",
        "    tokens = [w for w in token if w.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "Japan['Token_Words'] = Japan.apply(identify_tokens, axis=1)\n",
        "\n",
        "\n",
        "def lemmatize_text(row):\n",
        "    Japan_5 = row['Token_Words']\n",
        "    lemma_list = [lemma.lemmatize(word, pos='v') for word in Japan_5]\n",
        "    return lemma_list\n",
        "\n",
        "\n",
        "Japan['lemma_words'] = Japan.apply(lemmatize_text, axis=1)\n",
        "\n",
        "\n",
        "def remove_stopwords(row, is_lower_case=False):\n",
        "    Japan_6 = row['lemma_words']\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in Japan_6 if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in Japan_6 if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "\n",
        "Japan['No_Stopwords'] = Japan.apply(remove_stopwords, axis=1)\n",
        "\n",
        "\n",
        "def remove_special_characters(row, remove_digits=False):\n",
        "    Japan_4 = row['No_Stopwords']\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', Japan_4)\n",
        "    return text\n",
        "\n",
        "\n",
        "Japan['No_sp_char'] = Japan.apply(remove_special_characters, remove_digits=True, axis=1)\n",
        "\n",
        "\n",
        "def remove_accented_chars(row):\n",
        "    Japan_3 = row['No_sp_char']\n",
        "    text = unicodedata.normalize('NFKD', Japan_3).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "\n",
        "Japan['No_Accented'] = Japan.apply(remove_accented_chars, axis=1)\n",
        "\n",
        "Japan['Lower_words'] = Japan['No_Accented'].str.lower()\n",
        "Japan.to_csv('Preprocessed_Covid_Britain.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HNbHrzsnjpW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbPUl_mpnjpX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqZkDCwanjpX"
      },
      "source": [
        "# For Headlines\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from contractions import CONTRACTION_MAP\n",
        "import unicodedata\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemma = WordNetLemmatizer()\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "Japan = pd.read_csv(\"India_Headline.csv\")\n",
        "\n",
        "# CONTRACTION_MAP = {\"I'm\":\"I am\"} # contraction definition. This is just an example, please change it here with your contractions\n",
        "Japan[\"Headline\"] = Japan[\"Headline\"].apply(lambda x:str(x).replace(\"’\",\"'\")) # replace the wrong quotation mark by the correct one\n",
        "for contraction in CONTRACTION_MAP:\n",
        "    Japan[\"Headline\"] = Japan[\"Headline\"].apply(lambda x:str(x).replace(contraction, CONTRACTION_MAP[contraction])) # in this case I'm just replacing the contraction by the expanded form. I iterate it through all the possible contractions\n",
        "\n",
        "\n",
        "def strip_html_tags(row):\n",
        "    Japan_1 = row['Headline']\n",
        "    soup = BeautifulSoup(Japan_1, \"html.parser\")\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "Japan['No_HTML'] = Japan.apply(strip_html_tags, axis=1)\n",
        "\n",
        "\n",
        "def identify_tokens(row):\n",
        "    articles = row['No_HTML']\n",
        "    token = nltk.word_tokenize(articles)\n",
        "    tokens = [w for w in token if w.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "Japan['Token_Words'] = Japan.apply(identify_tokens, axis=1)\n",
        "\n",
        "\n",
        "def lemmatize_text(row):\n",
        "    Japan_5 = row['Token_Words']\n",
        "    lemma_list = [lemma.lemmatize(word, pos='v') for word in Japan_5]\n",
        "    return lemma_list\n",
        "\n",
        "\n",
        "Japan['lemma_words'] = Japan.apply(lemmatize_text, axis=1)\n",
        "\n",
        "\n",
        "def remove_stopwords(row, is_lower_case=False):\n",
        "    Japan_6 = row['lemma_words']\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in Japan_6 if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in Japan_6 if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "\n",
        "Japan['No_Stopwords'] = Japan.apply(remove_stopwords, axis=1)\n",
        "\n",
        "\n",
        "def remove_special_characters(row, remove_digits=False):\n",
        "    Japan_4 = row['No_Stopwords']\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', Japan_4)\n",
        "    return text\n",
        "\n",
        "\n",
        "Japan['No_sp_char'] = Japan.apply(remove_special_characters, remove_digits=True, axis=1)\n",
        "\n",
        "\n",
        "def remove_accented_chars(row):\n",
        "    Japan_3 = row['No_sp_char']\n",
        "    text = unicodedata.normalize('NFKD', Japan_3).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "\n",
        "Japan['No_Accented'] = Japan.apply(remove_accented_chars, axis=1)\n",
        "\n",
        "Japan['Lower_words'] = Japan['No_Accented'].str.lower()\n",
        "Japan.to_csv('Preprocessed_Headline_Ind.csv', index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVUMg4jlnjpY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}